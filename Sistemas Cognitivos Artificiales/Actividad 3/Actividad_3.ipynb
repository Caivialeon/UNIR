{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yKYAEOUB70s"
   },
   "source": [
    "# Actividad 3: RNN y sus aplicaciones en las series temporales\n",
    "\n",
    "En esta actividad se va a aplicar el conocimiento adquirido sobre las RNN para entrenar modelos que sean capaces de predecir el comportamiento de las series temporales. Para ello, se usará un dataset de temperaturas para mediante la aplicación de RNN, predecir los valores futuros que tendrá la serie temporal que se tiene. Este trabajo se suele hacer mediante modelos ARIMA, pero en esta práctica se verá cómo el modelado mediante RNN es una opción muy buena en estos casos de series temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXqE6Z2ZGM6I"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW24cRGrCrXZ"
   },
   "source": [
    "**1. Descargar el dataset y almacenarlo**\n",
    "\n",
    "En primer lugar hay que importar tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loO19CfCYC2V",
    "outputId": "6d912119-b933-43c2-b2b5-86cf56d280e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZTs4Cd0C49G"
   },
   "source": [
    "El siguiente paso es importar las bibliotecas numpy y matplotlib. Además, se define el método **plot_series** que se utilizará para hacer las gráficas de las series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gLNhTOGEYDbQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jbJP279DJha"
   },
   "source": [
    "A continuación se descarga el dataset de las temperaturas mínimas diarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsXBJEeqYFbn",
    "outputId": "dd50b66f-d902-403f-d215-c3afec7dcb79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv \\\n",
    "    -O /tmp/daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jufo30y2DOIS"
   },
   "source": [
    "En este paso, se utilizará la biblioteca csv de Python para guardar y poder leer el dataset de temperaturas mínimas diarias que ha sido descargado en el paso anterior. Además, se construye la variable **series** que será donde se guarde la serie temporal. Por último, siempre que se trate con una serie temporal, es una buena práctica hacer un gráfico para poder verla y tener una idea de cómo es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUBTwlu82nNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "no85DGrbYHUM",
    "outputId": "b84d01a6-0a36-4ed9-d6b6-0a7310770be3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/daily-min-temperatures.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5020/932532783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtemps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tmp/daily-min-temperatures.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m   \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/daily-min-temperatures.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "time_step = []\n",
    "temps = []\n",
    "\n",
    "with open('/tmp/daily-min-temperatures.csv') as csvfile:\n",
    "  reader = csv.reader(csvfile, delimiter=',')\n",
    "  next(reader)\n",
    "  step=0\n",
    "  for row in reader:\n",
    "    temps.append(float(row[1]))\n",
    "    time_step.append(step)\n",
    "    step = step + 1\n",
    "\n",
    "series = np.array(temps)\n",
    "time = np.array(time_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyejfTbWL5Km"
   },
   "source": [
    "# 2. Creación de las variables necesarias para el diseño de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtvcOIk1Dxod"
   },
   "source": [
    "Una técnica muy común cuando se trata con series temporales es utilizar una ventana temporal que se vaya desplazando sobre la serie temporal para reducir su análisis a lo que ocurre en ese ventana de forma local, para a continuación realizar el modelado global.\n",
    "\n",
    "**Ejercicio 1 (0.4 puntos)**: Crear las variables de entrenamiento y validación y hacer la partición de las mismas. Las variables que hay que crear son:\n",
    "\n",
    "*   time_train\n",
    "*   x_train\n",
    "*   time_valid\n",
    "*   x_valid\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjiQ6hCWYI47"
   },
   "outputs": [],
   "source": [
    "## variables para la técnica de la ventana temporal\n",
    "\n",
    "#x es el eje, y time seria / valid es Test\n",
    "\n",
    "split_time = 2500\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "## Split del dataset en entrenamiento y validación\n",
    "\n",
    "## tu código para la creación de las 4 variables del ejercicio 1 aquí\n",
    "#listo#\n",
    "##Train set\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "\n",
    "##Test set\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk5l4wEPFmDM"
   },
   "source": [
    "2. Creación del método **windowed_datset** para poder utilizarlo en el modelo. Las entradas por parámetros del método son:\n",
    "\n",
    "*   series\n",
    "*   window_size\n",
    "*   batch_size\n",
    "*   shuffle_buffer\n",
    "\n",
    "El resto de elementos que se usan para construir la función ventana temporal para explorar el dataset, son métodos de Python para tratar con series temporales.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxzoP0UXL_jP"
   },
   "source": [
    "# 3. Diseño de la función para predecir los siguientes valores de la serie temporal usando la técnica de la ventana temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40vSdZ7pYK6S"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpTqdDfYGf_W"
   },
   "source": [
    "A continuación, y usando como modelo el método **windowed_dataset** se procederá a adaptar el método **model_forecast** que se usará para predecir los siguientes valores de la serie temporal utilizando la técnica de la ventana temporal.\n",
    "\n",
    "**Ejercicio 2 (1.6 puntos)**: completar el método model_forecast creando los elementos necesarios dentro del método:\n",
    "\n",
    "1.   Crear la variable **ds** y darle el valor resultante del método **from_tensor_slices** pasando por parametro **series** **(0.4 puntos)**\n",
    "2.   Actualizar la ventana (**window**) de la variable **ds** (nota: en este caso el tamaño es el mismo de la ventana, no es necesario que sea window_size+1) **(0.4 puntos)**\n",
    "3.   Crear el **flat_map** de la variable, teniendo en cuenta que el tamaño es **window_size** **(0.4 puntos)**\n",
    "4.   Añadir la siguiente linea de código: ds = ds.batch(32).prefetch(1)\n",
    "5.   Crear la variable **forecast** en la que se usará el método **predict** **(0.4 puntos)**\n",
    "6.   Por último, se devolverá la variable forecast.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eYlrylwYNqa"
   },
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "## tu código para el método model_forecast del ejercicio 2 aquí\n",
    "   ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "   ds = ds.window(window_size, shift=1,drop_remainder=True)\n",
    "   ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "   ds = ds.batch(32).prefetch(1)\n",
    "   forecast = model.predict(ds)\n",
    "   return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY2SZ_GsJBx4"
   },
   "source": [
    "A continuación, se limpia la sesión de keras, y se inicializan las variables necesarias para poder diseñar el modelo de series temporales a entrenar usando RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAUtfBB9JR2p"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "window_size = 64\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mOXrfFVMKRA"
   },
   "source": [
    "# 4. Diseño de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6YG9YclJk9n"
   },
   "source": [
    "**Ejercicio 3.1 (0.5 puntos)**: Hay que crear la variable **train_set** dandole el valor que se reciba del método **windowed_datset**, los parametros que debe recibir este método son: **x_train, window_size, batch_size, shuffle_buffer_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqgS7tu5JUc5"
   },
   "outputs": [],
   "source": [
    "## tu código aquí para el ejercicio 3\n",
    "train_set = windowed_dataset(x_train,window_size,batch_size,shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgrrsFHxKVWO"
   },
   "source": [
    "**Ejercicio 3.2 (4 puntos)**: Se debe construir la red neuronal de aprendizaje profunda basada para modelar la serie temporal de las temperaturas minimas diarias. Esta red neuronal debera contar con las siguientes capas ocultas:\n",
    "\n",
    "1.   Una capa de convolución en una dimensión que tenga 32 filtros, una tamaño del kernel de 5, un stride de 1, padding \"causal\", la función de activación debe ser relu y el input shape debe ser [None, 1] \n",
    "2.   Una capa LSTM con 64 neuronas y retorno de secuencias \n",
    "3.   Una capa LSTM con 64 neuronas y retorno de secuencias \n",
    "4.   Una capa densa con 30 neuronas\n",
    "5.   Una capa densa con 10 neuronas\n",
    "6.   Una capa densa con 1 neuronas\n",
    "7.   Por último, se añade la siguiente capa: tf.keras.layers.Lambda(lambda x: x * 400)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yswRAtQ6JWvd"
   },
   "outputs": [],
   "source": [
    "## tu código para la red neuronal del ejercicio 4 aquí\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(32,5,1,'causal',activation='relu',input_shape=[None,1]),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True),\n",
    "    tf.keras.layers.Dense(30,activation='relu'),\n",
    "    tf.keras.layers.Dense(10,activation='relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Lambda(lambda x: x*400)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3VhCNbVLyPj"
   },
   "source": [
    "# 5. Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvaUY4TmMUpa"
   },
   "source": [
    "**Ejercicio 4 (0.5 puntos)**: Se va a diseñar un método callbacks para el learning rate que será guardado en la variable **lr_schedule**, este método deberá usar el método **LearningRateScheduler** de Python y será una función **lambda** que le de el valor a epoch de 1e-8 * 10** (epoch / 20) **texto en negrita** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8YmUm3WJZdm"
   },
   "outputs": [],
   "source": [
    "## tu código para crear la variable lr_schedule aquí\n",
    "#Se modifico la formula de acuerdo a la clase de repaso.\n",
    "#lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8*10*(epoch/20))\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8*10**(epoch/20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6xI_87qNgzg"
   },
   "source": [
    "**Ejercicio 5 (1.5 puntos):** Compilar la red neuronal con los siguientes parametros:\n",
    "\n",
    "*   loss: método Huber de keras\n",
    "*   El optimizador debe ser el SGD con learning rate 1e-8 y momentum 0.9\n",
    "*   La métrica a visualizar es el error absoluto medio (medium absolute error en ingles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYCGYDj3OI97"
   },
   "outputs": [],
   "source": [
    "## tu código para compilar la red neuronal para el ejercicio 5 aquí\n",
    "model.compile(\n",
    "    loss='huber', \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-8,momentum=0.9), \n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYcl-m5_ORq0"
   },
   "source": [
    "Para terminar se entrena el modelo previamente diseñado y compilado en los pasos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8788Rd6YXDm",
    "outputId": "41f0de23-ddb7-4a5c-a93b-ae9e53c1e619"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wryNEsg3OQ-Q"
   },
   "source": [
    "# 6. Actualización del learning rate según los resultados obtenidos del primer entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEtS47TWOd8J"
   },
   "source": [
    "Después del entrenamiento de la red neuronal se ve que learning rate resultante es de 1e-5. Se visualizará gráficamente para entender el motivo por el que se ha usado ese valor. En la gráfica se puede ver cómo el learning rate con el que menos loss hay es 1e-5, y por ese motivo, se debe volver a entrenar la red neuronal con dicho learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "KpydHm4hYZap",
    "outputId": "9c6bd097-8c70-4a64-abda-1d0715c36256"
   },
   "outputs": [],
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJCCYtWUOo3R"
   },
   "source": [
    "Se vuelve a inicializar la sesión de entrenamiento y la variable train_set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRczRPRJO2hr"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhM4opUcO9W0"
   },
   "source": [
    "**Ejercicio 6 (0.5 puntos)**: Para crear el nuevo modelo, reutiliza la red neuronal diseñada en el ejercicio 4, pero esta vez utilizando 60 filtros en la capa de convolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGXpMjoxO4eK"
   },
   "outputs": [],
   "source": [
    "## tu código para la red neuronal del ejercicio 6 aquí\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(60,5,1,'causal',activation='relu',input_shape=[None,1]),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1), \n",
    "    tf.keras.layers.Lambda(lambda x: x*400)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd5wr58SPaet"
   },
   "source": [
    "**Ejercicio 7 (0.5 puntos)**: Se debe volver a compilar la red neuronal de manera análoga a la del ejercicio 5, pero esta vez utilizar un learning rate obtenido de la función callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0cVmEu5O7UZ"
   },
   "outputs": [],
   "source": [
    "## tu código para compilar la red neuronal para el ejercicio 7 aquí\n",
    "model.compile(\n",
    "    loss='huber', \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-5,momentum=0.9),\n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYrg5kSzYb5i",
    "outputId": "92f70ea3-e2fa-4015-80dd-378a88a06202"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_set,epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE59CkauRD_o"
   },
   "source": [
    "# 7. Predicción de los siguientes valores de la serie temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ati5x7OfReWH"
   },
   "source": [
    "Para concluir la actividad, se usa el método model_forecast que se ha diseñado utilizando el método de la ventana temporal para hacer el nuevo metodo rnn_forecast con el cual se calcularán los nuevos valores de la serie temporal. Posteriormente, se pinta una gráfica para ver esos resultados y comprobar de forma visual que son correctos. Además, se dan los resultados de esas predicciones en forma númerica, de esta forma, este modelo diseñado en esta actividad podría ser el input de un nuevo algoritmo si fuera necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWB79gRyYdvB"
   },
   "outputs": [],
   "source": [
    "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
    "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "OrMdV-SnYiCG",
    "outputId": "fac41b71-1055-423e-ceb0-bf17dc41a303"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, rnn_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpa88mPVYj27",
    "outputId": "3b0726c4-499a-4770-f532-a38ff2b1d2be"
   },
   "outputs": [],
   "source": [
    "tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4VPNDrEYmEA",
    "outputId": "d97eeb9a-311c-4f9d-d3de-2d0f07a49a6c"
   },
   "outputs": [],
   "source": [
    "print(rnn_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46vKm-K1RVrl"
   },
   "source": [
    "# 8. Mostrar gráficamente los resultados.\n",
    "\n",
    "Una vez obtenido el resultado de la actividad, se procede a revisr de forma gráfica el training y validation loss a lo largo de los epochs en este nuevo entrenamiento con el learning rate optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwQvZC6wlaTi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Recuperar una lista de resultados de la lista de datos de entrenamiento y pruebas para cada epoch de entrenamiento\n",
    "#-----------------------------------------------------------\n",
    "loss=history.history['loss']\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAEJ3vP3lbz9"
   },
   "source": [
    "A continuación se realiza el plot de la pérdida frente a los epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "yKwWOTYCl4Ib",
    "outputId": "9b70f9f0-f6da-4254-a3e6-ec77d502abab"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "# Pérdida de entrenamiento y validación por epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4817Otl6A6"
   },
   "source": [
    "**Ejercicio 8 (0.5 punto)**: Utilizando las 2 nuevas variables zoomed_loss y zoomed_epochs y con base en el código anterior, hacer el plot del loss frente a los epochs entre los epoch 20 y 150 para ver como va oscilando y no es un proceso lineal como podria parecer según el anterior plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "gh6OToWdAggR",
    "outputId": "1445138d-cb2b-4c25-f3d8-2dc055d7eb7f"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "# Pérdida de entrenamiento y validación por epoch con zoom\n",
    "#------------------------------------------------\n",
    "zoomed_loss = loss[20:]\n",
    "zoomed_epochs = range(20,150)\n",
    "#\n",
    "## tu código para el plot con zoom del ejercicio 8 aquí\n",
    "plt.plot(zoomed_epochs, zoomed_loss, 'r')\n",
    "plt.title('loss')\n",
    "plt.xlabel(\"Zoomed Epochs\")\n",
    "plt.ylabel(\"Zoomed Loss\")\n",
    "#plt.legend([\"Zoomed Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJC-vsXFjZhd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
